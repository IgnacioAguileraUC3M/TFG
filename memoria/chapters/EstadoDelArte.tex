\section{Objetivos de Desarrollo Sostenible}

En 2015 la \gls{ONUg} (\gls{ONUa}) definió una serie de
objetivos con el fin de enfrentar y superar los principales problemas que
afectan a la humanidad hoy en día en un plazo de 15 años. Estos se conocen como
\gls{ODSg} (\gls{ODSa}) y conforman la denominada Agenda 2030,
la cual detalla el plan de acción a seguir para guiar el desarrollo humano en una
dirección más sostenible \cite{Agenda2023}.

En total fueron definidos 17 objetivos, cada uno desglosado en diferentes metas
a alcanzar. Estas metas establecen las bases sobre las cuales los gobiernos,
entidades privadas y organismos internacionales deben construir sus políticas
con con el fin de cumplir dichos objetivos. La colaboración entre las entidades
mencionadas es esencial para el éxito de la Agenda, tanto es así, que el último
de todos los \gls{ODSa} especifica de forma explícita la importancia de esta
cooperación \cite{JointSDGFund}.

De manera adicional, la Agenda 2030 detalla una serie de indicadores usados para
determinar de forma cuantitativa, objetiva y estandarizada el grado de
desarrollo de un país o institución con respecto a cada uno de los
objetivos \cite{UNSDGs}. Estos indicadores, junto con otras medidas proporcionadas por
análisis de expertos o soluciones basadas en \gls{IAg} (\gls{IAa})
permiten realizar un seguimiento efectivo de los objetivos. Mediante estas
técnicas es posible verificar si las políticas adoptadas por las entidades son
efectivas a la hora de alcanzar los objetivos o no.

Por ello, el desarrollo de herramientas e indicadores capaces de determinar de
forma objetiva la validez de las medidas adoptadas es de vital importancia.
Además, en el caso de no alcanzarse alguno de los objetivos, toda la información
obtenida mediante estos análisis permitirá delimitar los factores que causaron
el incumplimiento de la Agenda 2030. Anualmente, las Naciones Unidas publican en
un informe las investigaciones realizadas así como el análisis resultante de la
aplicación de los indicadores mencionados
anteriormente.\cite{UNSDGProgressReport}

Por su naturaleza, estos objetivos presentan una alta interconexión entre sí,
dando lugar a casos en los que avances positivos hacia un objetivo pueden
resultar en un impacto negativo hacia otro objetivo diferente. Esta
interconexión es un ámbito muy amplio de estudio y de suma importancia para
lograr entender el ecosistema de metas relacionadas que conforman los \gls{ODSa}
con el fin de poder proporcionar soluciones equilibradas que remen en una
dirección común.

Este trabajo, aun no teniendo como objetivo explorar estas
interrelaciones, puede emplearse para identificar estas conexiones subyacentes
entre diferentes objetivos, facilitando la elaboración de nuevos trabajos de 
investigación sobre la materia. 

A continuación se presentan cada uno de los 17
\gls{ODSa}, aportando información sobre los mismos \cite{UNSDGs}:

\begin{enumerate}
	\item \textbf{Fin de la pobreza:} Erradicar de manera efectiva la pobreza extrema y
	la desigualdad económica.
	\item \textbf{Hambre cero:} Garantizar y asegurar una nutrición adecuada, promoviendo
	una agricultura sostenible
	\item \textbf{Salud y bienestar:} Mejorar la salud de la gente, garantizando el
	acceso a un servicio sanitario competente 
	\item \textbf{Educación de calidad:} Asegurar una educación de calidad, inclusiva, y
	objetiva para todo el mundo.
	\item \textbf{Igualdad de género:} Erradicar la desigualdad de genero, empoderando a
	las mujeres y niñas.
	\item \textbf{Agua limpia y saneamiento:} Realizar un consumo sostenible del agua y
	garantizar el acceso a agua limpia.
	\item \textbf{Energía asequible y no contaminante:} Generar energía de manera verde y
	sostenible, garantizando el acceso a la misma.
	\item \textbf{Trabajo decente y crecimiento económico:} Garantizar un trabajo decente
	para todos y fomentar el crecimiento sostenible.
	\item \textbf{Industria, innovación e infraestructura:} Formar una industria,
	infraestructura y corrientes de investigación sostenibles
	\item \textbf{Reducción de las desigualdades:} Reducir las desigualdades entre países
	y dentro de los mismos.
	\item \textbf{Ciudades y comunidades sostenibles:} Lograr que las ciudades y los
	centros de población sean sostenibles.
	\item \textbf{Producción y consumo responsables:} Formar una cadena de producción
	sostenible, haciendo un uso eficiente de los recursos.
	\item \textbf{Acción por el clima:} Frenar y combatir el cambio climático
	\item \textbf{Vida submarina:} Conservar y hacer un uso responsable y sostenible de
	los mares y recursos marinos
	\item \textbf{Vida de ecosistemas terrestres:} Conservar y hacer un uso responsable y
	sostenible de los ecosistemas terrestres
	\item \textbf{Paz, justicia e instituciones sólidas:} Promover sociedades pacíficas y
	un sistema jurídico justo y accesible para todos.
	\item \textbf{Alianzas para lograr los objetivos:} Promover la cooperación y la
	formación de instituciones internacionales para cumplir los objetivos.
\end{enumerate}

%--------------------INTELIGENCIA ARTIFICIAL--------------------
\section{Inteligencia artificial}

La \gls{IAg} (\gls{IAa}) es la disciplina que compone todas
aquellas técnicas que permiten a un ordenador tomar decisiones informadas de
manera automática y precisa, imitando en mayor o menor medida las
características y funciones cognitivas propias del cerebro y la mente
humana \cite{McKinseyAI}. Se trata de un área de conocimiento extensa, en la cual se está
invirtiendo una gran cantidad de recursos, tanto en desarrollo como en
investigación, y se encuentra a la vanguardia tecnológica. Se trata de un área
de investigación relativamente antigua en relación con la computación, llegando
a preceder a los ordenadores como los conocemos hoy en día.

Dentro del estudio de la \gls{IAa} existen diferentes disciplinas de gran
relevancia actualmente como lo son el Aprendizaje Automático y el Aprendizaje
Profundo. El primero de ellos hace referencia a aquellas inteligencias
artificiales que son capaces de aprender con el tiempo basándose en unos datos
de ejemplo. Aprendizaje Profundo es a su vez un subconjunto de Aprendizaje
Automático en el que las máquinas están formadas por una serie de capas de
neuronas interconectadas, de manera análoga a la biología del cerebro humano
\cite{ieeeAI}.

El Aprendizaje Automático es un campo de estudio muy extenso, sin embargo su
objetivo radica en la generación de modelos matemáticos capaces de identificar
patrones en un conjunto de datos mediante un proceso de entrenamiento o
aprendizaje. Posteriormente, el modelo entrenado se emplea para proporcionar una
solución a un problema vinculado a dichos datos. Dentro del Aprendizaje
Automático se pueden hacer numerosas subdivisiones en diferentes disciplinas en
función de diversos criterios. Sin embargo, para este trabajo, el criterio que más procede es el que se realiza en base a la
naturaleza de los datos que se emplean como entrenamiento. Siguiendo por esta
línea, los diferentes conjuntos que conforman el campo del Aprendizaje
Automático son:

\begin{itemize}
	\item \textbf{Aprendizaje supervisado}: Estos modelos hacen uso de datos
\textit{en bruto}, sin  ningún tipo de información adicional, llamados no
etiquetados. Estos modelos necesitan realizar un análisis profundo para
identificar las características que diferencian a los datos entre si. Existen
diferentes aproximaciones que usar al la hora de tratar con datos no
etiquetados: 
	\begin{itemize}
		\item Modelos probabilísticos: usan funciones probabilísticas para
		inferir relaciones y características nuevas de los datos dados
		\item Modelos de agrupación: hacen uso de medidas de distancia para
		agrupar los datos en grupos independientes, revelando relaciones ocultas
		entre ellos.
	\end{itemize}
	\item \textbf{Aprendizaje no supervisado}: Suelen ser los más potentes pero
necesitan de datos etiquetados, datos que ya contienen la información que se
desea extraer de los mismos, de esta manera al presentarles datos nuevos serán
capaces de inferir esta información deseada en base a los datos dados para el
entrenamiento. Algunos son los siguientes
	\begin{itemize}
	\item Árboles de decisión: modelos en forma de árbol donde las salidas
	deseadas se encuentran en las hojas y un dato dado va iterando por cada rama
	eligiendo un camino u otro dependiendo de los atributos diferenciadores mas
	importantes hasta generar la salida deseada. Estos atributos y los valores
	que se usan como baremo para las ramificaciones son sacados de los datos de
	entrenamiento.
	\item Aprendizaje Profundo: probablemente el tipo de modelo más importante y que
	más en auge está. Su objetivo es imitar el comportamiento del cerebro
	humano, simulando las neuronas y sus conexiones para generar, en base al
	conocimiento almacenado en forma de datos, un agente inteligente. Tiene un
	problema importante que es la falta de interpretabilidad de los modelos,
	estos en la mayoría de los casos funcionan como una caja negra a la que se
	le introducen datos y genera salidas, pero el funcionamiento interno y el
	por qué del mismo muchas veces es incomprensible.
	\end{itemize}
\end{itemize}


\subsection{Historia de la IA}

El surgimiento de la \gls{IAa} suele enmarcarse en la década de los 40, cuando el
matemático Alan Turing conocido como el padre de la computación moderna planteó
el famoso \textit{Test de Turing} \cite{HarvardAIHistory}. En él se plantea si una máquina
pudiera llegar a pensar de forma autónoma e imitar el comportamiento humano.
Este test consiste en una intervención entre dos humanos y una máquina. Uno de
los dos humanos realiza preguntas de manera simultánea a los dos participantes
restantes, el otro humano y la máquina. Este tiene como objetivo determinar cual
de las dos respuestas ha sido generada por la máquina. Por otra parte, la
máquina tiene como objetivo hacerse pasar por humano, por lo tanto, si no es
reconocida como máquina superará la prueba. 

Fuera del marco teórico, la primera evidencia experimental que hay sobre la \gls{IAa}
tal y como la conocemos en la actualidad es la creación de la célula de
McCulloch Pitts Fig. \ref{Modelo_celula_McCulloch_Pitts}. Estas células, o
dispositivos, tienen dos estados posibles apagado (0) y encendido (1) y cuentan
con una serie de conexiones de entrada y una de salida, estas conexiones alteran
el valor que pasa por ellas acorde al peso de dicha conexión, de esta manera
simulan una versión simplificada de una neurona humana. Estas neuronas, aunque
simples suponen la base para el resto de modelos de Aprendizaje Automático.
\begin{figure}[ht]
    \includegraphics[scale=1]{CelulaMcCullochPitts.eps}
    \centering
	\captionsetup{justification=centering}
    \caption{Modelo célula McCulloch Pitts}
    \label{Modelo_celula_McCulloch_Pitts}
\end{figure}

El siguiente gran paso en la \gls{IAa} fue la creación del perceptrón multicapa, basado
en la interconexión de Células de McCulloch Pitts. Su creación supuso un gran
paso puesto que introdujo unos de los conceptos más importantes en el desarrollo
de Redes de Neuronas Artificiales, \textit{la Retropropagación}. En todos los
modelos basados en neuronas (Aprendizaje Profundo) el peso de las conexiones
necesita ajustarse a los datos de entrada, es esto lo que las dota de la
capacidad de aprender. En una red con una sola capa es trivial, primero se
calcula el error cometido en la fase de entrenamiento y posteriormente se
ajustan las conexiones acorde a ese error. Pero con más de una capa de neuronas,
el problema se vuelve más complejo puesto que no se sabe en que medida hay que
ajustar las conexiones de cada capa. Es este problema el que se soluciona con el
algoritmo de retropropagación, haciendo uso de funciones de error derivables
para inferir el error proveniente de cada capa. Este descubrimiento abrió las
puertas a la creación de arquitecturas neuronales cada vez más complejas y
potentes, encontrándose actualmente la limitación de estos modelos en su
complejidad computacional y la cantidad y calidad de los datos que se emplean
para el entrenamiento.

Después de la creación del perceptrón y el algoritmo de retropropagación, la
investigación en el ámbito de la \gls{IAa} atravesó un periodo de altibajos, los llamados
inviernos de la \gls{IAa}. Épocas en las que la investigación se ve detenida
bruscamente debido a barreras tecnológicas y, a su vez épocas doradas que surgen
a raíz de descubrimientos punteros que rompen esas barreras. Hoy en día gracias
a los avances en capacidad computacional y la gran cantidad de datos disponibles
nos encontramos en la época más fructífera de la \gls{IAa}. Esto está apoyado por la
creación de una serie de modelos basados en aprendizaje profundo altamente
especializados en tareas concretas. Las características y arquitecturas de estos
modelos están en gran medida influenciadas y determinadas por la naturaleza de
los problemas que se espera que resuelvan. Algunos de estos modelos importantes
son:


\subsubsection{Redes neuronales prealimentadas}

Conforman la familia de redes neuronales artificiales más sencillas, definida
por todas las redes de neuronas en las que la entrada se propaga en una sola
dirección, hacia adelante. Pese a que su definición hace que sea un término
aplicable a multitud de redes, es común utilizarlo a la hora de hacer referencia
a un perceptrón multicapa. 

Un perceptrón, como se ha mencionado anteriormente, es una de las primeras redes
diseñadas y la primera que podemos considerar compleja. Su arquitectura es muy
básica, consta de una serie de capas con un número arbitrario de neuronas en
cada una de ellas. Todas las neuronas de una capa están conectadas a todas las
neuronas de  siguiente. Es gracias a esta característica por la que estas redes
también son denominadas como \textit{Fully Connected}.

\begin{figure}[H]
	\includegraphics[scale=1.5]{imagenes/perceptron_multicapa.eps}
	\centering
	\captionsetup{justification=centering}
    \caption{Representación perceptrón multicapa}
    \label{Representación perceptrón multicapa}
\end{figure}

\subsubsection{Redes convolucionales}
Tienen como objetivo principal el procesar y tratar imágenes, es por esto por lo
que la red está formada por una serie de capas en dos dimensiones, representando
la imagen. Estas capas hacen uso de convoluciones para extraer los aspectos y
características generales más importantes de las imágenes dadas, es por esto por
lo que las denomina como \textit{capas de convolución} o
\textit{convolucionales}. Estas extraen la información reduciendo el tamaño de
la entrada cada vez más y eliminando las partes menos relevantes.
Posteriormente a estas capas convolucionales, la señal de la red, se ve
\textit{aplanada}, pasando de dos a una dimensión para poder servir como entrada
a un red prealimentada. Es esta última red la que procesa esta información
generando la salida deseada. 

Estas redes han demostrado ser increíblemente potentes y versátiles a la hora de
tratar con imágenes ya que las capas de convolución, al extraer la información
más relevante de cada imagen hace que identificar los patrones necesarios para
resolver el problema deseado sea una tarea más sencilla.

\begin{figure}[H]
	\includegraphics[scale=0.7]{imagenes/DIagramaRedConvolucional.eps}
	\centering
	\captionsetup{justification=centering}
    \caption{Representación red convolucional}
    \label{Representación red convolucional}
\end{figure}


\subsubsection{Redes recurrentes}
Son el primer tipo de redes que se salen de la definición de \textit{prealimentada}. Estas hacen usos de mecanismos que implementan características
similares a la memoria, incorporando como entrada a la red la salida generada
anteriormente, esto proporciona a la red datos contextuales, variando la salida
para una misma entrada dependiendo de este contexto.

% incorporando en las entradas de lso datos
% información correspondiente a salidas anteriores generadas por la red. Esto hace
% que para un mismo dato la salide varie dependiendo del contexto en el que se
% encuentre, más especificamente de los elementos anteriores a el.

Estas características hacen de las redes recurrentes excepcionales modelos a la
hora de tratar con datos secuenciales, es decir cuya información no solo esté
codificada en el dato en si, si no que esta dependa contexto que lo rodea. Se
usan especialmente en el ámbito del procesamiento del lenguaje natural, puesto
que el significado de una palabra varia dependiendo del contexto en el que se
encuentra. 

\begin{figure}[H]
	\includegraphics[scale=1.15]{imagenes/DIagramaRedRecurrente.eps}
	\centering
	\captionsetup{justification=centering}
    \caption{Representación red recurrente}
    \label{Representación red recurrente}
\end{figure}
\subsubsection{Mecanismos de atención}
% (https://insights.daffodilsw.com/blog/what-is-the-attention-mechanism-in-deep-learning)
% (https://www.youtube.com/watch?v=AIiwuClvH6k)
% (https://pdf.sciencedirectassets.com/271597/1-s2.0-S0925231221X00227/1-s2.0-S092523122100477X/
% main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEHIaCXVzLWVhc3QtMSJHMEUCIQDprEBMvQyJrBMAHfZqA4d7OOwAPdUZiO%
% 2FWXDK2jm7hQgIgXauizVdqw7fv11AXoaZWPUsTBAQ2HCcPuqdNVgqxAL4qswUIGhAFGgwwNTkwMDM1NDY4NjUiDHOwRHcgI7zPYUj19
% iqQBSLycJ2FyBWgWZBTceskAKw7Vv81OZP83bitOgq4LJeHPw2TPx8dYuef3isZCKaZ%2BFgH9DHrmGRtbRUbpLj5CUlcdeR4Kyjjqy
% BkYCWgIxmOQ1eaf0BwBlCfj1gewuzK2uLXabH5o1RmHgSdjxsOESykWz4rW48fI91xnNcIU5hucfasgTkoWjKJvH5uLnsp%2B%2FWS
% kYK162L9qRoaZPR%2Bpkfk3J%2BJH%2B4gMz87Xdz4Ma8vcrztDAKwxgC1PiaPLJf%2B7IZlbECbrQ%2BQFke%2B6Jg5B%2F%2Bouc
% 86i%2FDebiIUR8cdL5kQwAk135Xt1NFg0DwsjGCFViuOraOIFLTUbE6bi73uemFFHXkxp8cnOoVApkjyy1tdWy7ltUMAEKbdi3FUWB
% 79cflxpyuSI1EbWpYy9mLNKRNbsJ5vIu%2BGCA4KdHBJ%2FL3ZRK4rGPzs5ruZgzlt1Q4APxOB8l6dunK3UCOIDTJHG2kPYgLxaSl7
% FBE2upUf0TIXgKa75wzKrOYaPK71%2F8hLC%2B8m3qtc5f4DuQZ%2BPio%2BpLBCf3B8uOm9h%2BVHu9JbDamWgIKAAfsm2t2AkDEQ
% FN1olqf96gf5%2FPwyPiDmKUsEv76dVMbnqvQG2wFIFjNyyMs1Qk2e%2F%2FXyhjwT0kKPfLDLlpVwbdPd5DXJEIn%2B4WJ1c
% Fso9%2FzRyf21JMsqh8V9v2Ls33d3tKSfj4Uxq2NFltR%2FOKsL%2FSbaMETS2yKKARd%2BCNe%2B8cO3ggHzIx3RtEwbL%2Fjrg
% fqJ0DJ3cIHAyHwYmLv8m%2FWfVh6P1qUV5siNY%2BVBW6xFmMLKRO9rEIDXvmhaoEInX67Gq7fc2h7X0ZJYYvxMP6jOKNabz%2B
% jHAgxUNKFW1wWNiFFUfi1drQEezLKfE%2FKJMIfEr6YGOrEB6BXnXXrhRP3PyaI7DHCw6HDU4sbhUB0V%2BXxm04jnb3AgJg%2FoF
% 2uUu3zlIydvpz5v70wo6rMkzjXRhfYI0mEYjS0hna8HhCPhvv3B7%2FzbdrWgW5%2FSKYapDNpIBWGjab3Odx6PCt3MeTrbgWqWaIS
% rA0Q%2BfkWJTk5WxEoPEQiCN2xt%2FFReMJor61XUJ9R4mxR5e2S3MNeHVs7Weh7XugETB7r8hfzvJgXhRIuiQg1MZfF4&X-Amz-Al
% gorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230803T182812Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-
% Credential=ASIAQ3PHCVTY7TXNDVGM%2F20230803%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=6e03a077cad
% 527383285ceeb978fe43951b385b009c9fffcf752eb9fa006e9ca&hash=548b8b932eed70daa48d51ff6022b60aaab8b3312
% be837b8ade8773ce5415806&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pi
% i=S092523122100477X&tid=spdf-1aafefd3-5910-42fa-976e-f5f615eff882&sid=d0e007934778944fef69a8285560cdd
% 9f39cgxrqb&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0310580652575f065755&rr=7f109e770da23851&cc=es)

En los últimos años el termino \textit{Mecanismos de atención} ha cobrado una
notable importancia en el ámbito del aprendizaje profundo. Hace referencia a la aptitud
humana de centrarse  , o en mayor medida, en partes concretas de las
entradas (los sentidos), idealmente en las partes más relevantes. Esto es
especialmente importante ya que reduce notablemente la capacidad de
procesamiento necesaria para interpretar la información captada. Este concepto
aplica de manera directa a las redes de neuronas, haciendo que si estas
implementan mecanismos de atención, sean capaces de aprender en menos tiempo,
utilizando menos recursos y posiblemente ignorando el ruido que puedan contener
las entradas.  

La atención en el aprendizaje profundo se suele implementar como una matriz en
la que se relacionan, contextualmente, todos los elementos de la entrada entre
si. De esta manera, al procesar uno de estos elementos se puede identificar la
importancia que este tiene con respecto a todos los demás y prestar
\textbf{atención} a aquellos que están más relacionados. Esto difiere con los
modelos de redes recurrentes, ya que estos solo relacionan una entrada con la
anterior, implementando mecanismos de atención se puede tener una foto de las
relaciones entre todos los elementos.

Las ventajas que presentan estos mecanismos se ve aparente cuando se quieren
procesar frases como la siguiente: ``El pangolín dormía plácidamente colgado de
la rama de un árbol usando su cola.'' \cite{DotcsvAtencion}, en este caso una red recurrente, al
procesar la última parte de la frase ``usando su cola'' no podrá identificar a
quien se hace referencia. Por otra parte, un mecanismo de atención si podrá
identificar la relación entre ``El pangolín'' y ``usando su cola'', pudiendo
entender correctamente la frase. 

Estos mecanismos se utilizan a la par de redes recurrentes, dotándolas de
características de atención, haciendo de ellas unos modelos mucho más completos.
Es aquí donde surge unos de los avances más grandes de los últimos años en el
ámbito del aprendizaje profundo. Este surge en el 2017 con la publicación
\textit{``Attention is all you need``} \cite{vaswani2017attention} y supone un cambio en el paradigma
ya que lo autores de dicho artículo proponen una nueva arquitectura que
incorpora únicamente estos mecanismos de atención y prescinden de las redes
recurrentes, el llamado \textit{transformer}. Estos nuevos modelos, pese a
incorporar únicamente mecanismos de atención, presentan una arquitectura
compleja, constando de diferentes módulos que, conjuntamente, identifican la
información relevante de la entrada y posteriormente, haciendo uso de redes prealimentadas (al igual que con las redes convolucionales), procesan esta información
extraída por los mecanismos de atención y generan la salida deseada.

% \textbf{Meter mas vainainformación de transformers}

\subsection{Aprendizaje por transferencia}
Unos de los conceptos más importantes en la actualidad es el llamado
\textit{Transfer Learning} (Aprendizaje por transferencia), este hace referencia
a utilizar un modelo entrenado con un conjunto de datos inicial y que ha
obtenido buenos resultados y entrenarlo con un conjunto nuevo. Habitualmente ese
conjunto inicial está formado por datos generales para establecer una base de
conocimiento amplia para posteriormente adaptarla a un problema nuevo, de manera
más rápida y eficaz que si se entrenara únicamente con datos concretos, afines
al problema a resolver. 

Hoy en día tiene especial importancia ya que los modelos que mejores resultados
obtienen cuentan con arquitecturas complicadas y muy pesadas, los mencionados
transformers, estando formados por cientos de millones de parámetros variables.
Estos modelos solo pueden ser entrenados con conjuntos de datos masivos y en
ordenadores con una alta capacidad computacional y una amplia memoria
disponible. Es por esto por lo que no cualquiera puede generar y menos entrenar un
modelo de estas magnitudes por lo que entidades grandes los desarrollan para que
posteriormente puedan ser usados en un corto periodo de tiempo y haciendo uso de
conjuntos de datos menores, adaptados a las necesidades finales. 

Existen multitud de modelos disponibles con los que hacer aprendizaje por transferencia
dependiendo de la finalidad deseada, de todos ellos el más relevante en el marco
de este proyecto es:

\paragraph{\gls{BERTa} (\gls{BERTg})} Modelo
publicado en 2019 por Google es uno de los más utilizados para tareas
relacionadas con el tratamiento de texto. Como su nombre indica consta de una
arquitectura basada en transformers, pero cuenta con un factor diferenciador ya
que presenta una capacidad de atención bidireccional, esto significa que las
palabras se relacionan con las anteriores y posteriores.\cite{devlin2019bert} La potencia del modelo
surge de esta arquitectura y la manera en la que se ha entrenado, este
entrenamiento también es peculiar, contando con dos fases, cada una con el objetivo
de perfeccionar capacidades diferentes, estos dos entrenamientos son:
\begin{itemize}
	\item Masked LM: en este primer proceso de entrenamiento se ocultan ciertas
	palabras de la entrada de manera aleatoria, al rededor de un 15\%, y se
	entrena el modelo para que sea capaz de predecir cuales son estas palabras,
	basándose únicamente en el contexto que las rodea. El proceso de ocultado
	también es relevante ya que los elementos seleccionados no siempre se reemplazan
	con una cadena especifica que indica esta ocultación, si no que otras veces
	se reemplazan por otro elemento aleatorio y otras veces no se cambian pese a
	ser seleccionados. Esta parte proporciona un entrenamiento bidireccional.\cite{devlin2019bert}
	
	\item Next Sentence Prediction (NSP) "Predicción de la Siguiente Frase": en
	esta segunda fase se entrena el modelo para que sea capaz de predecir la
	siguiente frase, como los desarrolladores afirman, esta parte fortalece las
	capacidades de respuesta a preguntas y de inferencia del lenguaje natural
	del modelo. Para este entrenamiento se escogen dos frases consecutivas
	cualesquiera \textit{A y B} y se entrena a el modelo para que dado
	\textit{A} sea capaz de predecir \textit{B}. Este paso también cuenta con
	peculiaridades, ya que no todas las veces la frase \textit{B} seleccionada
	es realmente consecutiva a \textit{A}, concretamente en el 50\% de los casos
	es una frase aleatoria del corpus.\cite{devlin2019bert}
\end{itemize}

Estos dos tipos de entrenamientos hacen de \gls{BERTa} un modelo sorprendentemente
versátil, robusto y sorprendentemente fácil de usar, siendo necesario añadir una
única capa adicional para poder adaptarlo a virtualmente cualquier aplicación de
\gls{NLPa}, conteniendo esta última capa las neuronas necesarias para codificar la
salida deseada.

Otros modelos de \textit{aprendizaje por transferencia} comunes son:
\begin{itemize}
	\item \textbf{GPT (Generative Pre-trained Transformer)}: Los modelos GPT son
	arquitecturas Transformer preentrenadas que se utilizan para generar texto
	coherente y contextualmente relevante. Estos modelos son entrenados en
	grandes cantidades de texto y son capaces de generar texto en función de un
	contexto
	dado.

	\item \textbf{ResNet}: ResNet es una arquitectura de red neuronal
	convolucional que utiliza bloques residuales para permitir un entrenamiento
	efectivo de redes profundas. Esto es especialmente útil en tareas de visión
	por computadora donde se deben identificar objetos en imágenes.
	
	\item \textbf{InceptionNet}: InceptionNet, también conocida como GoogLeNet,
	es una arquitectura de red neuronal convolucional que utiliza módulos de
	convolución múltiple para capturar características de diferentes escalas en
	una imagen.
	
	\item \textbf{VGGNet}: VGGNet es una arquitectura de red neuronal
	convolucional que se destaca por su simplicidad y profundidad uniforme. Es
	conocida por su rendimiento en tareas de reconocimiento de objetos en
	imágenes.
	
	\item \textbf{Xception}: Xception es una arquitectura de red neuronal
	convolucional que se basa en la idea de separar las convoluciones espaciales
	y de profundidad. Esto ayuda a capturar detalles y características en
	imágenes de manera eficiente.
	
	\item \textbf{MobileNet}: MobileNet es una familia de arquitecturas de redes
	neuronales convolucionales diseñadas para ejecutarse eficientemente en
	dispositivos móviles y embebidos, lo que las hace ideales para aplicaciones
	con recursos limitados.
	
	\item \textbf{DenseNet}: DenseNet es una arquitectura de red neuronal
	convolucional que utiliza conexiones densas entre capas para permitir un
	flujo eficiente de información a través de la red. Esto ayuda en la
	identificación precisa de objetos en imágenes.
	
	\item \textbf{U-Ne}t: U-Net es una arquitectura de red neuronal utilizada en
	tareas de segmentación de imágenes, especialmente en aplicaciones médicas.
	Su estructura de codificador-decodificador es efectiva para resaltar áreas
	específicas en imágenes.
	
	\item \textbf{CycleGAN}: CycleGAN es un tipo de red generativa adversaria
	(GAN) que se utiliza para transformar imágenes de un dominio a otro, como
	convertir fotos en pinturas al estilo de un artista o cambiar el estilo de
	una imagen.
\end{itemize}

% \textbf{Igual meter algún dato de tests de bert}

% \paragraph{ResNet}
% \paragraph{Llama2}
% \begin{itemize}
% 	\item \gls{BERTa} (Bidirectional Encoder Representations from Transformers): uno de
% 	los modelos más usados, creado por Google y entrenado sobre un corpus
% 	formado por todas las entradas de Wikipedia y libros disponibles en formato
% 	digital. La manera exacta en la que se entrenó escapa el alcance de este
% 	proytecto pero su naturaleza basada en transformers lo hace especialmente
% 	útil a la hora de realizar tareas con textos.
% 	\item ResNet: \dots
% 	\item Llama2: \dots
% \end{itemize}


\section{Procesamiento del Lenguaje Natural}Dento de la \gls{IAa} todo el subgénero que
se dedica a tratar con texto se denomina \textit{Procesamiento del Lenguaje
Natural} o \gls{NLPa} por sus siglas en inglés. Esta disciplina tiene como
objetivo general abrir un canal de comunicación entre las máquinas y los
humanos, manteniendo la independencia de las partes comunicadoras. Para esto un
ordenador tiene que ser capaz de entender el lenguaje como lo haría un humano,
tanto para el procesamiento de las entradas textuales que le llegan como para, a
partir de esas u otras entradas de otros tipos, generar un texto comprensible
por un humano. Una vez un ordenador es capaz de entender texto natural se puede
entrenar para realizar tareas como la traducción, clasificación y generación
coherente de texto. \cite{StanfordSLP3}

Se ha mencionado el \textit{Lenguaje Natural}, pero es importante destacar la
diferencia entre este tipo de lenguajes y los formales, tales como el lógico o
matemático. Este primero surge de la necesidad humana de comunicarse, y
evoluciona como lo haría un organismo vivo para conseguir una comunicación lo
más completa y eficiente posible y cuenta con una serie de reglas que lo
definen pero no lo limitan y estas evolucionan en paralelo. Esto difiere de los
lenguajes formales en el hecho de que estos últimos tienen una serie de reglas
estrictas e invariantes que definen el uso que ha de hacerse del lenguaje y
cualquier variación o excepción no es aceptada.\cite{vasquez2009procesamiento}

Una vez definido el lenguaje natural solo queda interpretarlo, esto es un
problema ya que su representación no es matemática, por lo que un ordenador, y
en concreto y modelo de inteligencia artificial no seria capaz de procesarlo. Es
por esto que el primer paso es seguir algún proceso para representar de manera
matemática las entradas textuales. Este preprocesado requiere de varios pasos
en lo que se divide la entrada en diferentes partes o dosis de información
llamados \textit{tokens}, también se requiere de normalizar dichas
palabras, eliminando información irrelevante y dejando únicamente la raíz,
puesto que es la parte que contiene el mayor numero de información. Una vez
procesada la entrada se traduce a su representación matemática para poder ser
tratada por un ordenador. 

\subsubsection{Tokenizacion}
La primera de estar tareas es la tokenización, esta consiste en dividir la
entrada de texto en sus partes fundamentales. A priori parece una tarea
sencilla, dividir la entrada en sus diferentes palabras, realizando una división
en cada espacio. Esto, aunque intuitivo no siempre funciona, sobre todo en
inglés ya que muchas palabras se representan con apostrofes y tienen que ser
divididas. Es por esto que se hace uso de una serie de reglas y expresiones
regulares que tienen en cuenta todas
estas posibilidades y consiguen dividir de manera eficaz y eficiente el texto en
una serie de \textit{tokens}.

\begin{center}
	\textit{The house wasn't sold for \$500,000.00}

	\textit{The\_house\_was\_n't\_sold\_for\_\$\_500.000,00}
\end{center}

\subsubsection{Lematización}

El siguiente paso después de dividir la entrada es normalizarla, este proceso
consiste en estandarizar las entradas para que todas contengan información del
mismo tipo y estructura \cite{MicrosoftNormalizeData}. En el uso de números la
normalización consiste en transformar todas las entradas para que estas estén en un
rango entre 0 y 1, manteniendo las relaciones pero normalizando la magnitud. En
el ámbito de textos la normalización suele consistir en reducir las palabras a
su raíz, la llamada \textit{lematización}, haciendo que todas mantengan la
mayor parte de su significado pero eliminando las diferencias irrelevantes
entre ellas y haciendo que dos palabras diferentes con el mismo significado sean
virtualmente iguales \textit{(soy = eres)}. \cite{TextNormalizationNLP}

\begin{center}
	\textit{The house wasn't sold for \$500,000.00}

	\textit{The\_house\_be\_not\_sell\_for\_\$\_500.000,00}
\end{center}

\subsubsection{Vectorización}
El último paso es traducir la entrada ya preparada para que pueda ser utilizada
por un ordenador, esto se hace transformando cada palabra en un vector que la
representa y se denomina \textit{Word Embedding}. Esto se puede llevar a cabo
por medio de varios métodos pero actualmente uno de los más usados es el llamado
\textit{word2vec}\cite{text2vec,text2vecTF}. Este no es un método como tal, si
no dos técnicas usadas para entrenar modelos de aprendizaje automático con la
finalidad de vectorizar palabras, de tal manera que palabras similares se
encuentran en posiciones similares. Estas dos técnicas son
\textit{\gls{CBOWg} (\gls{CBOWa})} y \textit{\gls{SGg} (\gls{SGa})}, ambas definen métodos de entrenamiento para que los
modelos entrenados siguiendo esas pautas sean capaces de vectorizar tokens
correctamente, ambos tienen como objetivo predecir palabras pero de maneras
diferentes.
\begin{itemize}
	\item CBOW: en esta arquitectura se usan las palabras anteriores y
	posteriores (4 en cada dirección suele generar los mejores resultados) para intentar
	predecir la palabra del medio.
	\item SG: esta arquitectura funciona de manera inversa a la anterior, en
	esta se usa una palabra para intentar predecir ciertas palabras anteriores y
	ciertas posteriores
\end{itemize}

\begin{figure}[H]
	\includegraphics[scale=1]{imagenes/CBOWvsSG.eps}
	\centering
	\captionsetup{justification=centering}
	\caption{CBOW vs SG}
	\label{CBOW vs SG}
\end{figure}

Estas dos aproximaciones pueden implementarse por medio de varios modelos, en el
trabajo original los autores usan un clasificador \textit{log-lineal}, modelo
que hace uso de funciones para, a partir de las características de los datos,
generar distribuciones probabilísticas que posteriormente son usadas para
tareas de clasificación y predicción\cite{text2vec}. El hecho de trabajar en
escala logarítmica los hace especialmente eficaces a la hora de identificar y
modelar relaciones complejas entre los datos \cite{LogniearModels}. Una vez se
ha entrenado el modelo, este puede ser usado para generar un vector de la
dimensionalidad deseada que encapsula la información semántica de la palabra,
esta información surge a raíz del aprendizaje realizado en la tarea de
predicción.

El hecho de representar las palabras mediante vectores abre un amplio abanico de
herramientas de análisis, pudiendo hacer uso de operaciones algebraicas con
palabras. Estas operaciones nos permiten, entre otras cosas, calcular la
similitud entre palabras, partiendo de la base de que palabras similares tienen
vectores asociados que se encuentran en posiciones similares en el espacio, si
calculamos el producto escalar entre los vectores de dos palabras obtenemos como
resultado una medida de similitud entre las dos. Adicionalmente el hecho de
definir un espacio de alta dimensionalidad permite el cálculo de diferentes
medidas de similitud, dependiendo de las dimensiones y operaciones usadas.  

Otra característica de estos vectores es que pueden ser sumados y restados,
resultando en propiedades curiosas como por ejemplo la siguiente operación:
$v[rey] - v[hombre] + v[mujer]$. El resultado de esta operación es un vector
que se encuentra cerca de $v[reina]$. 

Una vez se ha procesado y vectorizado el texto a usar, se pueden usar los
modelos y técnicas definidos para realizar las tareas y solucionar los problemas
deseados.
% En este trabajo los autores
% proponen dos técnicas diferentes para entrenar modelos con la finalidad de que
% estos aprendan a vectorizar palabras

% este hace uso de redes feed
% forward (referencia interna?) entrenadas para, a aprtir de una palabra dada como
% entrada generar una salida compuesta por un vector de alta dimensionalidad, en
% la publicación se comparan varias dimensionalidades, pero el producto final
% genera vectores de una diomensión de entre 100 y 300
% (https://nlp.stanford.edu/pubs/glove.pdf), a partir de 300, el operar en
% espacios dimensionales mayores no brinda mejoras sustanciales. Dicha red feed
% forward está entrenada para generar 


\section{Clasificación multi etiqueta}
Una de las tareas más comunes es la de la clasificación, esta consiste en que el
modelo sea capaz de predecir a que clase pertenece un dato dado, algunos
ejemplos comunes son la clasificación de números y letras escritas a mano y la
clasificación de textos acorde al sentimiento que evoca. Estas tareas se pueden
clasificar en dos grupos en base al número de clases predecidas y los datos
usados:

\begin{figure}[H]
	\includegraphics[scale=0.35]{imagenes/ClasificacionImagenes.png}
	\centering
	\captionsetup{justification=centering}
	\caption{Clasificación números}
	\label{Clasificación números}
\end{figure}

\paragraph{Clasificación multiclase}
En este caso se desea clasificar a los datos en una de varias clases, de manera
que cada dato pertenece únicamente a una a la vez. Por ejemplo el caso
mencionado anteriormente de la clasificación de números, \cref{Clasificación números}, una única imagen puede
corresponder a un único numero. 


\paragraph{Clasificación multietiqueta}
En este caso el objetivo es clasificar cada dato en todas las clases a la que
pertenezca, pudiendo un dato pertenecer a varias a la vez, por ejemplo el caso de clasificación de textos de acorde al sentimiento que evocan, un texto puede tener un tono enfadado y
agresivo a la vez. \cite{MultiLabelClassification}

Cabe mencionar un último tipo de clasificación, la llamada \textit{clasificación
binaria}, en este tipo de tareas se quiere asignar uno de dos valores a una
entrada, positivo o negativo por lo que solo se cuenta con una clase.

\subsection{Función de activación}Una de las partes esenciales en las
neuronas de una red es la función de activación, esta transforma el valor que se
propaga por la red. En las neuronas de salida tienen un papel fundamental
siendo las responsables de generar una salida con el formato correcto e
interpretable.\cite{sharma2017activation}

En el caso de clasificación esta función de activación final puede ser de dos
tipos dependiendo del problema de clasificación \textit{sigmoid} y
\textit{softmax}.\cite{Liermann2019}

\paragraph{Sigmoid} Una de las funciones de activación más comunes ya que se
trata de una función no lineal, lo cual permite a las redes adaptarse a
problemas no lineales, y genera salidas entre 0 y 1. Estas salidas, en tareas de
clasificación, pueden ser tratadas como probabilidades por lo que la salida
se puede interpretar fácilmente, asignando a una entrada todas aquellas clases
cuya respectiva salida supera un determinado umbral. Este proceso aplica de
igual manera a las tareas de clasificación binaria, para las cuales se debe
contar con una única neurona de salida. \cite{NARAYAN199769}

\paragraph{Softmax} Esta función de activación está formada por una combinación
de funciones sigmoid que, al igual que esta, genera un vector con una
probabilidad por cada neurona de salida y por ende cada clase posible. La principal
diferencia entre las dos es que en este caso todas las probabilidades suman
100\% por lo que no son independientes. Esto hace de softmax la mejor función de activación para tareas de
clasificación multiclase, cuando se quiere asignar una única etiqueta a la
entrada, en este caso se asigna aquella etiqueta con la mayor probabilidad
asignada. \cite{sharma2017activation}. Los vectores usados para entrenar este tipo de redes se denominan como \textit{one-hot} \footnote{La codificación one-hot es una técnica de representación de datos en la que cada valor categórico se convierte en un vector binario único, se trata como un anglicismo debido a la falta de una traducción directa en español.}, teniendo un único elemento con un valor positivo de 1, mientras que el resto tiene un 0. El proceso de clasificación transforma el vector de salida en uno de este tipo, asignando un 1 a la clase con mayor porcentaje y un 0 al resto.


% Estre concepto tiene especial importancia cuando se habla de tareas de
% clasificación ya que es necesario interpretar correctamente la salida de una red
% para asignar las etiquetas correctas. En este ámbito existen dos fuinciones de
% activación predominantes, sigmoid y softmax. Ambas generan salidas numéricas
% acotadas entre 0 y 1 pero tienen una diferencia esencial, en el caso de softmax
% la suma de todas las salidas generadas es 1

% Una de las caracteristicas que hace de las redes de neuronas uno de los modelos
% más flexibles es la función de activación, esta define la forma de la salida
% generada por la red y permite transformar salidas lineales a salidas de otros
% . La finalidad de la red y los datos usados definen, en la
% mayoría de los casos la funcion a usar. Es por esto que hay una gran cantidad de
% funciones disponibles, pero en el ámbito de la clasificaciopn existen dos
% funciones principales: sigmoid y softmax. Ambas producen un vector de salida que
% codifica la probabilidad de pertenencia de cada clase (un número por casa clase
% posible) y es trabajo del creador del modelo determinar que criterio seguir para
% elegir la/las clases pertenecientes al dato.

% Estas dos funciones difieren en una caracteristica principal, softmax genera un
% vector de probabilidades que, al sumarlas todas, suman 1, por lo que es usado en
% tareas de clasificacion multi clase. Por otra parte una funcion de salida
% sigmoid genera un vector en el que todas las probabilidades son independientes,
% por lo que es usado en tareas de clasificación multi etiqueta, pero es necesario
% definir un umbral de tal manera que todas las clases con una probabilidad mayor
% que dicho umbral serán asignadas al dato de entrada. Este concepto se espandirá
% en apartados posteriores.

\section{Web Scraping}
Una vez definido el estado del arte referente a la \gls{IAg}, es
fácil deducir que obtener una buena cantidad de buenos datos es imperativo si se
desea obtener un buen modelo, para esto una de las técnicas más usada es la de
\textit{Web Scrapping} o \textit{raspado de web} según su traducción literal. Esta es una
técnica en la que se desarrollan agentes automáticos que navegan por internet
con la finalidad de extraer los datos deseados de uno o varios sitios web. Hay
muchas maneras de conseguir este objetivo, pero todas se engloban en dos grupos,
los que imitan las acciones que haría un humano, interactuando de manera
dinámica con una página por medio de un navegador y los que hacen uso de
peticiones \gls{httpa} simples y sus respectivas respuestas para inspeccionar el código
fuente. Estos últimos no hacen uso de navegadores por lo que extraen esa
información de manera más rápida y eficiente pero corren el riesgo de ser
bloqueados por algún sistema de control de trafico, adicionalmente, si el acceso
a los datos requiere de interacción con la página estos últimos agentes no son
capaces de recolectarla. \cite{zhao2017web}

\section{Tecnologías usadas}
En este trabajo se han hecho uso de las siguientes tecnologías y herramientas:

\paragraph{Python} Se ha seleccionado python como el lenguaje de programación
del proyecto por dos razones principales: actualmente el es lenguaje más
utilizado para la recolección y tratado de datos al igual que para realizar
trabajos de aprendizaje automático \cite{zhao2017web, EmeritusPythonDataScience,
CourseReportPythonDataScience} y es el lenguaje más usado durante la carrera ya
que se enseña en primero debido a su sencillez, potencia y versatilidad.

\paragraph{Librerías} Las diferentes librerías desarrolladas para python usadas
en el desarrollo de este trabajo son las siguientes:
\begin{itemize}
	\item \textbf{Tensorflow}: una de las librerías mas utilizadas para realizar tareas
	de aprendizaje automático, cuenta con multitud de herramientas para el
	desarrollo de modelos de aprendizaje profundo, al igual que facilitar el aprendizaje por transferencia, ofreciendo multitud de modelos de manera sencilla. 
	\item \textbf{Keras}: librería encapsulada en tensorflow pero de suma importancia ya
	que es la que implementa todas herramientas de alto nivel para construir y
	entrenar los modelos, al igual que para tratar los datos a usar en el entrenamiento.
	\item \textbf{Pandas}: Librería por defecto a la hora de tratar con grandes
	cantidades de datos, permitiendo el uso de estructuras llamadas
	\textit{DataFrames} que incorporan multitud de herramientas para el
	procesado y tratado eficiente de datos. 
	\item \textbf{Scikit-Learn}: Librería ampliamente utilizada en el ámbito del aprendizaje automático debido a la multitud de herramientas que proporciona 
	\item \textbf{Selenium}: Librería más usada a la hora de realizar web scrapping de
	manera interactiva, por medio de navegadores como \textit{chromium}.
	\item \textbf{Requests y Beautifulsoup4}: Librerías más usada para tareas de
	WebScrapping no interactivo, haciendo uso de requests para realizar las
	peticiones http junto a Beautifulsoup4 para interpretar las respuestas
	conteniendo el código fuente \gls{htmla} de la página facilitando la extracción de
	datos.
	\item \textbf{Bokeh}: Librería altamente versátil para la creación de gráficas con
	un alto nivel de personalización y la posibilidad de exportarlas en \gls{htmla} y
	javascript para implementarlas de manera interactiva en páginas web.
\end{itemize}  

\section{Trabajos similares}
Durante la fase de investigación se han encontrado multitud de trabajos y
herramientas cuya finalidad es similar a la de este trabajo, más en concreto el
desarrollo de una herramienta clasificadora que sea capaz de identificar los
objetivos de desarrollo sostenible mencionados y presentes en un texto. En esta
sección se detallan estos trabajos.

\paragraph{SDG meter}Proyecto desarrollado a finales de 2022 por Naciones
unidas, en concreto por el \gls{UNEPg} (\gls{UNEPa}), en el se desarrolla un modelo basado en \gls{BERTa} y una interfaz
web que lo encapsula y permite su utilización. En dicha plataforma el usuario
puede introducir un texto o archivo y, haciendo uso del modelo, se mostrarán los
\gls{ODSa}s relacionados con el texto dado y su respectivo porcentaje de pertenencia.
\cite{SDGmeter}

En su investigación explican las técnicas usadas, las cuales son similares a las
descritas en este trabajo ya que realizan un aprendizaje de clasificación multi-etiqueta, por medio del uso de \gls{BERTa} como base para su modelo y \textit{Web Scrapping}
para recolectar los datos. Destacar que no describen en detalle el desarrollo
llevado a cabo, indicando que usaron el modelo \gls{BERTa} al que se le añadió una capa
de salida, por la naturaleza del problema descrito se puede deducir que se trata
de una capa con 17 neuronas de salida, y debido a que se trata de un problema de
clasificación multi-etiqueta, una capa con función de activación softmax. Esta
última afirmación no encaja con los resultados mostrados, si la última capa
contara con una función de activación softmax se esperaria que los porcentajes
predecidos por el modelo fueran relativamente altos para algunos casos que no se
espera que el modelo clasifique, y los casos que el modelo si se espera que
clasifique deberían tener asignado un porcentaje alto, idílicamente más de un
50\%. En el caso descrito por los autores asignan predicciones como positivos
verdaderos cuando estas tienen un porcentaje de un 1\% dado por el modelo. 

En general no explican en mucho detalle la metodología seguida ni la manera en
la que interpretan los resultados ni calculan la precisión del modelo, de todas
formas afirman haber obtenido un 98\% de precisión.

\paragraph{SDG detector}Proyecto publicado por \textit{\gls{JOSSg} (\gls{JOSSa})} en 2023 y desarrollado por miembros de diversas
universidades se trata de una herramienta creada para el lenguaje de
programación R, comúnmente usado en el ámbito de la estadística y análisis de
datos, que permite identificar a nivel de frases dentro de un corpus textual los
\gls{ODSa}s relacionados, junto con las respectivas metas. Siguen una metodología de
consultas basadas en palabras clave, similar a las consultas realizadas en bases
de datos. \cite{Li2023}

Reportan una precisión de al rededor de un 75\%, teniendo en cuenta que se
clasifican las metas y que cuenta con una resolución de clasificación a nivel de
frase, es un buen resultado, estando en lo que se suele considerar como el
umbral de lo aceptable.  

% \paragraph{SDG pathfinder} https://sdg-pathfinder.org/
\paragraph{OSDG} Plataforma alojada en una web que hace
uso de varias técnicas para signar objetivos a un texto, ya sea en formato plano
o \gls{PDFg} (\gls{PDFa}), también cuenta con una \gls{APIg} (\gls{APIa}) disponible para clasificar grandes cantidades
de datos. Esta herramienta, desarrollada en 2020, en su versión 2.0 cuenta con
dos clasificadores independientes, uno basado en una ontología que hace uso de
una serie de palabras clave para realizar la tarea de clasificación y uno
implementado en esta última versión 2.0 basado en técnicas de Aprendizaje Automático,
un objetivo será clasificado por esta herramienta si ambos clasificadores
coinciden en su pertenencia. \cite{OSDG1,OSDG2,OSDGTool}

Pese a su capacidad de clasificar datos de manera masiva por medio de una \gls{APIa}, y
puesto que ya se ha usado con esos fines, los autores no proporcionan ningúna
métrica de precisión del modelo por lo que su fiabilidad no es conocida.

\paragraph{Linked SDG}Producto desarrollado por el \gls{UN DESAg} (\gls{UN DESAa}), consiste en una página web en
la que introduce una página web o un archivo del que se extrae una serie de
datos relacionados con los objetivos de desarrollo sostenible, las diferentes
zonas geográficas presentes en el texto y los \gls{ODSa}s y metas más relevantes en el
texto. \cite{UNSDGLinkedSDGs,UNSDGAbout}

\paragraph{text2sdg}Paquete para el lenguaje R desarrollado por miembros de la
universidad de Basel en septiembre de 2022. Dicho paquete hace uso de diferentes
modelos y técnicas existentes basados en consultas para clasificar un texto
según los objetivos más relacionados, en estas consultas se busca la presencia
de determinados términos, si todos o algunos se cumplen, dependiendo de la
estructura de la consulta, se asignarán los objetivos determinados al texto
sobre el que esta se ha realizado, similar a la metodología seguida por los
autores de SDG detector. \cite{text2sdg1, text2sdg2, text2sdgWeb}

\paragraph{TFG UPM}Trabajo de fin de grado realizado por un estudiante de la
\gls{UPMg} (\gls{UPMa}) en 2021. En dicho trabajo se usa un
modelo \gls{LDAg} (\gls{LDAa}) para realizar la clasificación. Este modelo
proporciona el nivel de relevancia de cada objetivo con respecto al texto
proporcionado, no se realiza una clasificación per se asignando de manera
categórica objetivos al texto. Finalmente en la fase de pruebas y análisis se
proporciona una precisión del modelo de al rededor del 76\%, 9 textos
clasificados correctamente como los más relevantes, 4 como relevantes y 4 no
identificados como relevantes. Como se destaca en el trabajo las pruebas
realizadas son de una magnitud pequeña como para sacar conclusiones sobre el
nivel de precisión del modelo pero sirve como un punto de referencia para
futuras herramientas. \cite{upm68099}

\paragraph{Clasificación de objetivos con BERT}Uno de los trabajos relacionados con la
clasificación de textos de acorde a los \gls{ODSa}s más completos. Publicado en 2022,
en el los autores desarrollan un modelo basado en \gls{BERTa} capaz de predecir los
objetivos relacionados con un texo con una precisión del 94\%, medida excelente
para una tarea de clasificación con 17 etiquetas. En el realizan un
entrenamiento exhaustivo de \gls{BERTa} en una tarea de clasificación multi etiqueta.
\cite{matsui2022natural}

\paragraph{Multi label SDG classification}Artículo publicado por investigadores de
la universidad de Bologna en 2020 tiene como objetivo principal desarrollar una
herramienta que pueda agilizar el proceso de análisis de documentos acorde a los
\gls{ODSa}s por parte de Naciones Unidas. El modelo desarrollado consiste en una serie
de algoritmos de vectorización, \gls{TF-IDFg}  (\gls{TF-IDFa}, \gls{GloVeg} (\gls{GloVea})  y \gls{USEg} (\gls{USEa}  para calcular la similitud
entre un texto a clasificar y multitud de textos representativos de los
diferentes objetivos, el llamado corpus, esta similitud se calcula combinando la
\textit{Similitud coseno} \footnote{La similitud coseno es una medida que evalúa cuán parecidos son dos vectores o documentos, calculando el coseno del ángulo entre ellos.} de los diferentes algoritmos de
vectorización. Una vez calculada la similitud del texto a clasificar con
respecto a todos los textos en el corpus hacen uso de un umbral de similitud
clasificando aquellos objetivos con una similitud superior al umbral. En su fase
de pruebas reportan una precisión de entre el 90\% y 60\% dependiendo del tamaño
del conjunto de datos y su naturaleza. \cite{DBLP:journals/corr/abs-2004-03455}

Resulta un trabajo sumamente interesante y versátil ya que, debido a que hace
uso de medidas de similitud, el modelo puede ser fácilmente adaptado a nuevas
corrientes en la taxonomía de los objetivos simplemente actualizando el corpus.
